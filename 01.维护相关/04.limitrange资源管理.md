# limitrange资源管理

默认情况下container和pod会以无限制的cpu和memory资源运行，意味着pod和container可能使用系统上的所有资源。通过配置limitrange，对namespace下的pod和container进行全局控制，可实现CPU和Memory资源的管理。

## 配置limitrange

```bash
[root@k8s01 ~]#  vi nsdev-limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: nsdev-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 500m
    defaultRequest:
      memory: 128Mi
      cpu: 100m
    min:
      memory: 128Mi
      cpu: 100m
    max:
      memory: 1Gi
      cpu: 1000m
    maxLimitRequestRatio:
      cpu: 5
      memory: 5
    type: Container
  - max:
      memory: 2Gi
      cpu: 2
    min:
      memory: 128Mi
      cpu: 100m
    maxLimitRequestRatio:
      cpu: 5
      memory: 5
    type: Pod
```

* pod和container都可以设置min、max、maxLimitRequestRatio参数，只有container可以设置default和defaultRequest；
* 当type为container时，min为container resource request的下限，max为container resource limit的上限。当container未定义resource request和limit时，default值将作为spec.containers.resources.limits值，defaultRequest值将作为spec.containers.resources.requests值。对同一类型的资源，min<=defaultRequest<=default<=max；
* pod的min为pod中所有容器request值总和的下限，max为pod中所有容器limit值总和上限；
* 如果设置了container的max值，那么集群中的所有容器都必须设置该资源的limits，或使用default limit值，否则无法创建。如果设置了container的min值，那么集群中所有容器都不许设置该资源的requests值，或使用defaultRequest，否则无法创建。
* maxLimitRequestRatio定义了container和pod中limit和request值的最大比值。
* 如果容器设置了limits而没有设置requests值，那么系统将requests值设置等于limits值。

```bash
[root@k8s01 ~]# kubectl create -f nsdev-limitrange.yaml --namespace=nsdev
limitrange "nsdev-limit-range" created
[root@k8s01 ~]# kubectl describe limits -n nsdev
Name:       nsdev-limit-range
Namespace:  nsdev
Type        Resource  Min    Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---    ---  ---------------  -------------  -----------------------
Container   memory    128Mi  1Gi  128Mi            512Mi          5
Container   cpu       100m   1    100m             500m           5
Pod         cpu       100m   2    -                -              5
Pod         memory    128Mi  2Gi  -                -              5

```

## 测试limitrange

### 测试默认值：

```bash
[root@k8s01 ~]# kubectl run nginx --image=nginx --replicas=1 --namespace=nsdev
deployment.apps "nginx" created
[root@k8s01 ~]# kubectl describe -n nsdev pod nginx
Name:           nginx-65899c769f-6m7wj
Namespace:      nsdev
Node:           k8s02/172.16.1.11
Start Time:     Thu, 10 May 2018 16:14:57 +0800
Labels:         pod-template-hash=2145573259
                run=nginx
Annotations:    kubernetes.io/limit-ranger=LimitRanger plugin set: cpu, memory request for container nginx; cpu, memory limit for container nginx
Status:         Pending
IP:
Controlled By:  ReplicaSet/nginx-65899c769f
Containers:
  nginx:
    Container ID:
    Image:          nginx
    Image ID:
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sgmbn (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          False
  PodScheduled   True
Volumes:
  default-token-sgmbn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-sgmbn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              9s    default-scheduler  Successfully assigned nginx-65899c769f-6m7wj to k8s02
  Normal  SuccessfulMountVolume  8s    kubelet, k8s02     MountVolume.SetUp succeeded for volume "default-token-sgmbn"
  Normal  Pulling                8s    kubelet, k8s02     pulling image "nginx"
```

* 由于该pod未配置资源requests和limits，因此container用默认值。

### 测试limit：

```bash
[root@k8s01 ~]# vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  contianers:
  - name: webapp
    image: tomcat
    resources:
      limits:
        cpu: 2
        memory: 2Gi
    ports:
    - containerPort: 8080
[root@k8s01 ~]# kubectl create -f pod.yaml  -n nsdev
Error from server (Forbidden): error when creating "pod.yaml": pods "webapp" is forbidden: [maximum cpu usage per Container is 1, but limit is 2., maximum memory usage per Container is 1Gi, but limit is 2Gi.]
```

* 由于spec.contianer.resources.limits超过limitrange的max值，因此无法创建pod。

### 测试maxLimitRequestRatio：

```bash
[root@k8s01 ~]# vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - name: webapp
    image: tomcat
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 128Mi
    ports:
    - containerPort: 8080
[root@k8s01 ~]# kubectl create -f pod.yaml -n nsdev
Error from server (Forbidden): error when creating "pod.yaml": pods "webapp" is forbidden: [cpu max limit to request ratio per Container is 5, but provided ratio is 10.000000., memory max limit to request ratio per Container is 5, but provided ratio is 8.000000., cpu max limit to request ratio per Pod is 5, but provided ratio is 10.000000., memory max limit to request ratio per Pod is 5, but provided ratio is 8.000000.]
```

* 由于cpu和memory的limit和request比值都超过5，因此无法创建pod。

```bash
[root@k8s01 ~]# vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - name: webapp
    image: tomcat
    resources:
      limits:
        cpu: 400m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
    ports:
    - containerPort: 8080
[root@k8s01 ~]# kubectl create -f pod.yaml  -n nsdev
pod "webapp" created
[root@k8s01 ~]# kubectl describe pod webapp -n nsdev
Name:         webapp
Namespace:    nsdev
Node:         k8s03/172.16.1.12
Start Time:   Thu, 10 May 2018 16:31:03 +0800
Labels:       app=webapp
Annotations:  <none>
Status:       Pending
IP:
Containers:
  webapp:
    Container ID:
    Image:          tomcat
    Image ID:
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     400m
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sgmbn (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          False
  PodScheduled   True
Volumes:
  default-token-sgmbn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-sgmbn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              47s   default-scheduler  Successfully assigned webapp to k8s03
  Normal  SuccessfulMountVolume  47s   kubelet, k8s03     MountVolume.SetUp succeeded for volume "default-token-sgmbn"
  Normal  Pulling                47s   kubelet, k8s03     pulling image "tomcat"

```

* 由于定义了limits和requests，因此不会使用默认的default limit和defaultRequest；
* cpu和memory资源的maxLimitRequestRatio都为4，因此正常创建资源。

## 资源限制机制

### CPU资源

* CPU为可压缩资源；
* 空闲CPU资源按容器的Requests值的比例分配，例如容器A的CPU配置为Requests 1 limits 10，容器B的CPU配置为Requests 2 limits 8，A和B同时运行于同一节点，初始状态系统可用的资源为3 Cores，那么A和B恰好得到两容器requests定义的CPU资源。如果A和B都需要更多的CPU资源，恰好此时其他任务释放出1.5Cores CPU资源，那么1.5 Cores将按照A和B容器的Requests比例1:2分配给A和B，最终A得到1.5 Cores，B得到3 Cores；
* 如果POD使用了超过Limits 10中配置的CPU使用率，那么cgroups会对Pod中容器的CPU使用限流；如果Pod没配置Limits10，那么Pod会尝试抢占所有空闲的CPU资源。

### 内存资源

* 内存资源为不可压缩资源；
* Pod可以得到requests配置的内存。如果Pod使用的内存量小于它的requests值，那么这个pod可以正常运行(除非出现操作系统级别的内存不足等严重问题)；如果Pod使用的内存量超过它的requests值，而且少于limits值，那么这个Pod有可能被杀死。例如：Pod A内存使用超过requests但少于limits，此时同一节点另一个Pod B之前内存使用远少于requests值，但负载开始加大，Pod B向系统申请的内存资源不超过requests值的内存，而系统资源不足，系统将可能直接杀掉Pod A；另一种情况是Pod A使用了超过requests并少于limits值的内存，此时k8s将一个新的Pod调度到该节点上，新的Pod需要内存资源，k8s也可能杀死Pod A来释放资源；
* 如果Pod使用的内存量超过他的limits值，那么操作系统将杀掉Pod所有容器所有进程中使用内存最多的一个，直到内存不超过limits为止。
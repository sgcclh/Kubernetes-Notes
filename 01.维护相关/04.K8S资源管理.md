# K8S资源管理

Kubernetes通过limitrange对单个namespace中的Pod和容器进行全局控制，通过Resourcequota资源配额限制监控单个namespace中的资源使用。

## limitrange资源管理

默认情况下container和pod会以无限制的cpu和memory资源运行，意味着pod和container可能使用系统上的所有资源。通过配置limitrange，对namespace下的pod和container进行全局控制，可实现CPU和Memory资源的管理。

### 配置limitrange

```bash
[root@k8s01 ~]#  vi nsdev-limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: nsdev-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
      cpu: 500m
    defaultRequest:
      memory: 128Mi
      cpu: 100m
    min:
      memory: 128Mi
      cpu: 100m
    max:
      memory: 1Gi
      cpu: 1000m
    maxLimitRequestRatio:
      cpu: 5
      memory: 5
    type: Container
  - max:
      memory: 2Gi
      cpu: 2
    min:
      memory: 128Mi
      cpu: 100m
    maxLimitRequestRatio:
      cpu: 5
      memory: 5
    type: Pod
```

* pod和container都可以设置min、max、maxLimitRequestRatio参数，只有container可以设置default和defaultRequest；
* 当type为container时，min为container resource request的下限，max为container resource limit的上限。当container未定义resource request和limit时，default值将作为spec.containers.resources.limits值，defaultRequest值将作为spec.containers.resources.requests值。对同一类型的资源，min<=defaultRequest<=default<=max；
* pod的min为pod中所有容器request值总和的下限，max为pod中所有容器limit值总和上限；
* 如果设置了container的max值，那么集群中的所有容器都必须设置该资源的limits，或使用default limit值，否则无法创建。如果设置了container的min值，那么集群中所有容器都不许设置该资源的requests值，或使用defaultRequest，否则无法创建。
* maxLimitRequestRatio定义了container和pod中limit和request值的最大比值。
* 如果容器设置了limits而没有设置requests值，那么系统将requests值设置等于limits值。

```bash
[root@k8s01 ~]# kubectl create -f nsdev-limitrange.yaml --namespace=nsdev
limitrange "nsdev-limit-range" created
[root@k8s01 ~]# kubectl describe limits -n nsdev
Name:       nsdev-limit-range
Namespace:  nsdev
Type        Resource  Min    Max  Default Request  Default Limit  Max Limit/Request Ratio
----        --------  ---    ---  ---------------  -------------  -----------------------
Container   memory    128Mi  1Gi  128Mi            512Mi          5
Container   cpu       100m   1    100m             500m           5
Pod         cpu       100m   2    -                -              5
Pod         memory    128Mi  2Gi  -                -              5

```

### 测试limitrange

测试默认值：

```bash
[root@k8s01 ~]# kubectl run nginx --image=nginx --replicas=1 --namespace=nsdev
deployment.apps "nginx" created
[root@k8s01 ~]# kubectl describe -n nsdev pod nginx
Name:           nginx-65899c769f-6m7wj
Namespace:      nsdev
Node:           k8s02/172.16.1.11
Start Time:     Thu, 10 May 2018 16:14:57 +0800
Labels:         pod-template-hash=2145573259
                run=nginx
Annotations:    kubernetes.io/limit-ranger=LimitRanger plugin set: cpu, memory request for container nginx; cpu, memory limit for container nginx
Status:         Pending
IP:
Controlled By:  ReplicaSet/nginx-65899c769f
Containers:
  nginx:
    Container ID:
    Image:          nginx
    Image ID:
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sgmbn (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          False
  PodScheduled   True
Volumes:
  default-token-sgmbn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-sgmbn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              9s    default-scheduler  Successfully assigned nginx-65899c769f-6m7wj to k8s02
  Normal  SuccessfulMountVolume  8s    kubelet, k8s02     MountVolume.SetUp succeeded for volume "default-token-sgmbn"
  Normal  Pulling                8s    kubelet, k8s02     pulling image "nginx"
```

* 由于该pod未配置资源requests和limits，因此container用默认值。

测试limit：

```bash
[root@k8s01 ~]# vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  contianers:
  - name: webapp
    image: tomcat
    resources:
      limits:
        cpu: 2
        memory: 2Gi
    ports:
    - containerPort: 8080
[root@k8s01 ~]# kubectl create -f pod.yaml  -n nsdev
Error from server (Forbidden): error when creating "pod.yaml": pods "webapp" is forbidden: [maximum cpu usage per Container is 1, but limit is 2., maximum memory usage per Container is 1Gi, but limit is 2Gi.]
```

* 由于spec.contianer.resources.limits超过limitrange的max值，因此无法创建pod。

测试maxLimitRequestRatio：

```bash
[root@k8s01 ~]# vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - name: webapp
    image: tomcat
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 128Mi
    ports:
    - containerPort: 8080
[root@k8s01 ~]# kubectl create -f pod.yaml -n nsdev
Error from server (Forbidden): error when creating "pod.yaml": pods "webapp" is forbidden: [cpu max limit to request ratio per Container is 5, but provided ratio is 10.000000., memory max limit to request ratio per Container is 5, but provided ratio is 8.000000., cpu max limit to request ratio per Pod is 5, but provided ratio is 10.000000., memory max limit to request ratio per Pod is 5, but provided ratio is 8.000000.]
```

* 由于cpu和memory的limit和request比值都超过5，因此无法创建pod。

```bash
[root@k8s01 ~]# vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - name: webapp
    image: tomcat
    resources:
      limits:
        cpu: 400m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
    ports:
    - containerPort: 8080
[root@k8s01 ~]# kubectl create -f pod.yaml  -n nsdev
pod "webapp" created
[root@k8s01 ~]# kubectl describe pod webapp -n nsdev
Name:         webapp
Namespace:    nsdev
Node:         k8s03/172.16.1.12
Start Time:   Thu, 10 May 2018 16:31:03 +0800
Labels:       app=webapp
Annotations:  <none>
Status:       Pending
IP:
Containers:
  webapp:
    Container ID:
    Image:          tomcat
    Image ID:
    Port:           8080/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     400m
      memory:  512Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sgmbn (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          False
  PodScheduled   True
Volumes:
  default-token-sgmbn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-sgmbn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              47s   default-scheduler  Successfully assigned webapp to k8s03
  Normal  SuccessfulMountVolume  47s   kubelet, k8s03     MountVolume.SetUp succeeded for volume "default-token-sgmbn"
  Normal  Pulling                47s   kubelet, k8s03     pulling image "tomcat"

```

* 由于定义了limits和requests，因此不会使用默认的default limit和defaultRequest；
* cpu和memory资源的maxLimitRequestRatio都为4，因此正常创建资源。

## 资源限制机制

### CPU资源

* CPU为可压缩资源；
* 空闲CPU资源按容器的Requests值的比例分配，例如容器A的CPU配置为Requests 1 limits 10，容器B的CPU配置为Requests 2 limits 8，A和B同时运行于同一节点，初始状态系统可用的资源为3 Cores，那么A和B恰好得到两容器requests定义的CPU资源。如果A和B都需要更多的CPU资源，恰好此时其他任务释放出1.5Cores CPU资源，那么1.5 Cores将按照A和B容器的Requests比例1:2分配给A和B，最终A得到1.5 Cores，B得到3 Cores；
* 如果POD使用了超过Limits 10中配置的CPU使用率，那么cgroups会对Pod中容器的CPU使用限流；如果Pod没配置Limits10，那么Pod会尝试抢占所有空闲的CPU资源。

### 内存资源

* 内存资源为不可压缩资源；
* Pod可以得到requests配置的内存。如果Pod使用的内存量小于它的requests值，那么这个pod可以正常运行(除非出现操作系统级别的内存不足等严重问题)；如果Pod使用的内存量超过它的requests值，而且少于limits值，那么这个Pod有可能被杀死。例如：Pod A内存使用超过requests但少于limits，此时同一节点另一个Pod B之前内存使用远少于requests值，但负载开始加大，Pod B向系统申请的内存资源不超过requests值的内存，而系统资源不足，系统将可能直接杀掉Pod A；另一种情况是Pod A使用了超过requests并少于limits值的内存，此时k8s将一个新的Pod调度到该节点上，新的Pod需要内存资源，k8s也可能杀死Pod A来释放资源；
* 如果Pod使用的内存量超过他的limits值，那么操作系统将杀掉Pod所有容器所有进程中使用内存最多的一个，直到内存不超过limits为止。

## Resource QOS

### QOS级别

* QoS级别，Guaranteed(完全可靠)>Burstable(弹性波动，较可靠)>BestEffort(尽力而为)，级别越高，服务优先级越高。

### Guaranteed级别

```bash
[user01@k8s01 ~]$ vi qospod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: nsdev
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "500m"
      requests:
        memory: "200Mi"
        cpu: "500m"
[user01@k8s01 ~]$ kubectl create -f qospod.yaml
[user01@k8s01 ~]$ kubectl describe pod qos-demo
Name:         qos-demo
Namespace:    nsdev
Node:         k8s01/172.16.1.10
Start Time:   Thu, 10 May 2018 21:42:28 +0800
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.253.51.6
Containers:
  qos-demo-ctr:
    Container ID:   docker://89154de8aeab1c514d66aa3d3ff56ad19ab2d7dfee845a053ad68d79796c3ae8
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:0fb320e2a1b1620b4905facb3447e3d84ad36da0b2c8aa8fe3a5a81d1187b884
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 10 May 2018 21:44:25 +0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  200Mi
    Requests:
      cpu:        500m
      memory:     200Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sgmbn (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  default-token-sgmbn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-sgmbn
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              4m    default-scheduler  Successfully assigned qos-demo to k8s01
  Normal  SuccessfulMountVolume  4m    kubelet, k8s01     MountVolume.SetUp succeeded for volume "default-token-sgmbn"
  Normal  Pulling                4m    kubelet, k8s01     pulling image "nginx"
  Normal  Pulled                 2m    kubelet, k8s01     Successfully pulled image "nginx"
  Normal  Created                2m    kubelet, k8s01     Created container
  Normal  Started                2m    kubelet, k8s01     Started container
```

* cpu和memory requests和limits设置相同，创建pod后，将QoS Class设置为Guaranteed；
* 如果cpu和memory 只设置相同的limits，不设置requests，k8s将自动将requests设置为limits值，将QoS Class设置为Guaranteed。

### Burstable级别

```bash
[user01@k8s01 ~]$ vi burstable.yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-2
  namespace: nsdev
spec:
  containers:
  - name: qos-demo-2-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "128Mi"
[user01@k8s01 ~]$ kubectl create -f burstable.yaml
[user01@k8s01 ~]$ kubectl describe pod qos-demo-2
Name:         qos-demo-2
Namespace:    nsdev
Node:         k8s03/172.16.1.12
Start Time:   Thu, 10 May 2018 21:59:05 +0800
Labels:       <none>
Annotations:  kubernetes.io/limit-ranger=LimitRanger plugin set: cpu request for container qos-demo-2-ctr; cpu limit for container qos-demo-2-ctr
Status:       Running
IP:           10.253.25.2
Containers:
  qos-demo-2-ctr:
    Container ID:   docker://31bf6a0d2697b9fdee88b65c4d512846bdf47d243bfdb34c022a7583b40f586c
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:0fb320e2a1b1620b4905facb3447e3d84ad36da0b2c8aa8fe3a5a81d1187b884
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 10 May 2018 21:59:14 +0800
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  200Mi
    Requests:
      cpu:        100m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sgmbn (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  default-token-sgmbn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-sgmbn
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              1m    default-scheduler  Successfully assigned qos-demo-2 to k8s03
  Normal  SuccessfulMountVolume  1m    kubelet, k8s03     MountVolume.SetUp succeeded for volume "default-token-sgmbn"
  Normal  Pulling                1m    kubelet, k8s03     pulling image "nginx"
  Normal  Pulled                 53s   kubelet, k8s03     Successfully pulled image "nginx"
  Normal  Created                53s   kubelet, k8s03     Created container
  Normal  Started                53s   kubelet, k8s03     Started container
```

* 未达到QoS Class Guaranteed要求，Pod中至少有一个容器设置了requests值。

### BestEfford级别

```bash
[user01@k8s01 ~]$ vi bestefford.yaml
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-3
  namespace: nsdev
spec:
  containers:
  - name: qos-demo-3-ctr
    image: nginx
[user01@k8s01 ~]$ kubectl config use-context ctx-prod
Switched to context "ctx-prod".
[user01@k8s01 ~]$ kubectl create -f bestefford.yaml
[user01@k8s01 ~]$ kubectl describe pod qos-demo-3
Name:         qos-demo-3
Namespace:    nsprod
Node:         k8s03/172.16.1.12
Start Time:   Thu, 10 May 2018 22:06:43 +0800
Labels:       <none>
Annotations:  <none>
Status:       Running
IP:           10.253.25.2
Containers:
  qos-demo-3-ctr:
    Container ID:   docker://bcd76577afbf509339654ee679538f49902051e119d4fc5197a0a6dc0b5b70d5
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:0fb320e2a1b1620b4905facb3447e3d84ad36da0b2c8aa8fe3a5a81d1187b884
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Thu, 10 May 2018 22:06:49 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-69f2h (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  default-token-69f2h:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-69f2h
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     <none>
Events:
  Type    Reason                 Age   From               Message
  ----    ------                 ----  ----               -------
  Normal  Scheduled              32s   default-scheduler  Successfully assigned qos-demo-3 to k8s03
  Normal  SuccessfulMountVolume  32s   kubelet, k8s03     MountVolume.SetUp succeeded for volume "default-token-69f2h"
  Normal  Pulling                31s   kubelet, k8s03     pulling image "nginx"
  Normal  Pulled                 26s   kubelet, k8s03     Successfully pulled image "nginx"
  Normal  Created                26s   kubelet, k8s03     Created container
  Normal  Started                26s   kubelet, k8s03     Started container
```

* Bestefford QoS Class要求无任何cpu和memory requests和limits设置；
* 由于nsdev namespace之前设置了limitrange，设置了默认requests和limits值，因此先切换到nsprod namespace的上下文进行创建测试pod。

### QoS工作特性

* 当系统资源不足时，cpu requests无法得到满足，由于CPU为可压缩资源，容器得到的CPU资源会被压缩限流；
* 内存为不可压缩资源，当系统内存不足时，Bestefford级别将优先被杀死，但由于没设置limits值，因此系统空闲时也能获得更多的资源；当系统内存不足时，又无Bestefford级别Pod可杀死，这时Burstable级别Pod将被杀死。一般来说Guaranteed级别Pod优先级最高，一般不会被杀死，但系统确实没有其他优先级更低pod可被杀死释放资源时，Guaranteed级别Pod也可能被杀死。

## 资源配额

### 参考文档：

[资源配额-中文版](https://kubernetes.io/cn/docs/concepts/policy/resource-quotas/)

[资源配额-英文版](https://kubernetes.io/docs/concepts/policy/resource-quotas/)

### 资源配额介绍

* 资源配额，通过 ResourceQuota对象来定义， 对每个namespace的资源消耗总量提供限制。 它可以按类型限制namespace下可以创建的对象的数量，也可以限制可被该项目以资源形式消耗的计算资源的总量。
* 分配计算资源时，每个容器可以为CPU或内存指定请求和约束。也可以设置两者中的任何一个。如果配额中指定requests.cpu或 requests.memory 的值，那么它要求每个进来的容器针对这些资源有明确的请求。  如果配额中指定了 limits.cpu 或 limits.memory的值，那么它要求每个进来的容器针对这些资源指定明确的约束。

### 创建资源配额

```bash
[root@k8s01 ~]# vi resourcequota.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: nsprod-resource-quota
spec:
  hard:
    pods: "3"
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
[root@k8s01 ~]# vi resourcequota.yaml
[root@k8s01 ~]# kubectl create -f resourcequota.yaml -n nsprod
resourcequota "nsprod-resource-quota" created
[root@k8s01 ~]# vi resourcequota02.yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-counts
spec:
  hard:
    configmaps: "10"
    persistentvolumeclaims: "4"
    replicationcontrollers: "20"
    secrets: "10"
    services: "10"
    services.loadbalancers: "2"
[root@k8s01 ~]# kubectl create -f resourcequota02.yaml -n nsprod
resourcequota "object-counts" created
[root@k8s01 ~]# kubectl describe resourcequota -n nsprod
Name:            nsprod-resource-quota
Namespace:       nsprod
Resource         Used  Hard
--------         ----  ----
limits.cpu       0     2
limits.memory    0     2Gi
pods             0     3
requests.cpu     0     1
requests.memory  0     1Gi

Name:                   object-counts
Namespace:              nsprod
Resource                Used  Hard
--------                ----  ----
configmaps              0     10
persistentvolumeclaims  0     4
replicationcontrollers  0     20
secrets                 1     10
services                0     10
services.loadbalancers  0     2
```

* 在nsprod namespace空间创建resourcequota。

### 测试资源配额

```bash
[root@k8s01 ~]# kubectl run nginx --image=nginx --replicas=1 --namespace=nsprod
deployment.apps "nginx" created
[root@k8s01 ~]# kubectl describe deployment nginx -n nsprod
Name:                   nginx
Namespace:              nsprod
CreationTimestamp:      Fri, 11 May 2018 17:03:16 +0800
Labels:                 run=nginx
Annotations:            deployment.kubernetes.io/revision=1
Selector:               run=nginx
Replicas:               1 desired | 0 updated | 0 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:  run=nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type             Status  Reason
  ----             ------  ------
  Progressing      True    NewReplicaSetCreated
  Available        True    MinimumReplicasAvailable
  ReplicaFailure   True    FailedCreate
OldReplicaSets:    <none>
NewReplicaSet:     nginx-65899c769f (0/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  9s    deployment-controller  Scaled up replica set nginx-65899c769f to 1
[root@k8s01 ~]# kubectl get rs -n nsprod
NAME               DESIRED   CURRENT   READY     AGE
nginx-65899c769f   1         0         0         3m
[root@k8s01 ~]# kubectl describe rs nginx-65899c769f -n nsprod
Name:           nginx-65899c769f
Namespace:      nsprod
Selector:       pod-template-hash=2145573259,run=nginx
Labels:         pod-template-hash=2145573259
                run=nginx
Annotations:    deployment.kubernetes.io/desired-replicas=1
                deployment.kubernetes.io/max-replicas=2
                deployment.kubernetes.io/revision=1
Controlled By:  Deployment/nginx
Replicas:       0 current / 1 desired
Pods Status:    0 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  pod-template-hash=2145573259
           run=nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type             Status  Reason
  ----             ------  ------
  ReplicaFailure   True    FailedCreate
Events:
  Type     Reason        Age               From                   Message
  ----     ------        ----              ----                   -------
  Warning  FailedCreate  3m                replicaset-controller  Error creating: pods "nginx-65899c769f-8k299" is forbidden: failed quota: nsprod-resource-quota: must specify limits.cpu,limits.memory,requests.cpu,requests.memory
[root@k8s01 ~]# kubectl delete deployment nginx -n nsprod
```

* Replicas set创建失败，由于未设置limits.cpu,limits.memory,requests.cpu,requests.memory。

```bash
[root@k8s01 ~]# kubectl create -f resourcequota.yaml -n nsdev
resourcequota "nsprod-resource-quota" created
[root@k8s01 ~]# kubectl create -f resourcequota02.yaml -n nsdev
resourcequota "object-counts" created
[root@k8s01 ~]# kubectl run nginx --image=nginx --replicas=1 --namespace=nsdev
deployment.apps "nginx" created
[root@k8s01 ~]# kubectl describe deployment -n nsdev
Name:                   nginx
Namespace:              nsdev
CreationTimestamp:      Fri, 11 May 2018 22:32:51 +0800
Labels:                 run=nginx
Annotations:            deployment.kubernetes.io/revision=1
Selector:               run=nginx
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  1 max unavailable, 1 max surge
Pod Template:
  Labels:  run=nginx
  Containers:
   nginx:
    Image:        nginx
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-65899c769f (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  20s   deployment-controller  Scaled up replica set nginx-65899c769f to 1
[root@k8s01 ~]# kubectl describe resourcequota -n nsdev
Name:            nsprod-resource-quota
Namespace:       nsdev
Resource         Used   Hard
--------         ----   ----
limits.cpu       500m   2
limits.memory    512Mi  2Gi
pods             1      3
requests.cpu     100m   1
requests.memory  128Mi  1Gi

Name:                   object-counts
Namespace:              nsdev
Resource                Used  Hard
--------                ----  ----
configmaps              0     10
persistentvolumeclaims  0     4
replicationcontrollers  0     20
secrets                 1     10
services                0     10
services.loadbalancers  0     2
```

* 给nsdev namespace定义资源配额，由于nsdev namespace limitrange已经定义了容器的默认requests和limits，因此在nsdev namespace创建nginx deployment不会报错；
* 查看nsdev namespace的resourcequota，可以看到资源配额的使用情况。

### 资源配额作用域

```bash
[root@k8s01 ~]# kubectl create namespace qtscope
namespace "qtscope" created
[root@k8s01 ~]# vi scope.yaml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: scope
spec:
  hard:
    pods: "3"
  scopes:
  - BestEffort
[root@k8s01 ~]# kubectl create -f scope.yaml -n qtscope
resourcequota "scope" created
[root@k8s01 ~]# vi scope01.yaml 
apiVersion: v1
kind: ResourceQuota
metadata:
  name: scope01
spec:
  hard:
    pods: "3"
  scopes:
  - NotBestEffort
[root@k8s01 ~]# kubectl describe resourcequota -n qtscope
Name:       scope
Namespace:  qtscope
Scopes:     BestEffort
 * Matches all pods that do not have resource requirements set. These pods have a best effort quality of service.
Resource  Used  Hard
--------  ----  ----
pods      0     3

Name:       scope01
Namespace:  qtscope
Scopes:     NotBestEffort
 * Matches all pods that have at least one resource requirement set. These pods have a burstable or guaranteed quality of service.
Resource  Used  Hard
--------  ----  ----
pods      0     3
```

* 创建两个resourcequota，scope分别为besteffort和notbesteffort，限制都为3个pod。

```bash
[root@k8s01 ~]# kubectl run best-effort-nginx --image=nginx --replicas=2 --namespace=qtscope
deployment.apps "best-effort-nginx" created
[root@k8s01 ~]# kubectl describe resourcequota -n qtscope
Name:       scope
Namespace:  qtscope
Scopes:     BestEffort
 * Matches all pods that do not have resource requirements set. These pods have a best effort quality of service.
Resource  Used  Hard
--------  ----  ----
pods      2     3


Name:       scope01
Namespace:  qtscope
Scopes:     NotBestEffort
 * Matches all pods that have at least one resource requirement set. These pods have a burstable or guaranteed quality of service.
Resource  Used  Hard
--------  ----  ----
pods      0     3
[root@k8s01 ~]# kubectl run not-best-efford-nginx --image=nginx --replicas=3 --requests=cpu=100m --namespace=qtscope
deployment.apps "not-best-efford-nginx" created
[root@k8s01 ~]# kubectl describe resourcequota -n qtscope
Name:       scope
Namespace:  qtscope
Scopes:     BestEffort
 * Matches all pods that do not have resource requirements set. These pods have a best effort quality of service.
Resource  Used  Hard
--------  ----  ----
pods      2     3

Name:       scope01
Namespace:  qtscope
Scopes:     NotBestEffort
 * Matches all pods that have at least one resource requirement set. These pods have a burstable or guaranteed quality of service.
Resource  Used  Hard
--------  ----  ----
pods      3     3
```

* 创建best-efford-nginx deployment，由于未设置requests和limits值，因此QoS为bestefford，replicas为2，创建成功后，占用2个pod；
* 创建not-best-efford-nginx deployment，由于设置了cpu requests值，因此QoS为Burstable，属于NotBestEffort，replicas为3，创建成功后，占用3个pods。